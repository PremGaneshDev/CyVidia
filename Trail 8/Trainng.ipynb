{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Load your Excel files for training and validation\n",
    "train_file_path = '/Users/PremGanesh/Developer/AI/CyVidia/Input_Data/Training Dataset 2.xlsx'\n",
    "# load the excel file as a dataframe\n",
    "train_df = pd.read_excel(train_file_path) \n",
    "print(train_df.shape) # print the shape of the dataframe\n",
    "model_name = 'trained_model_rbi_jll_nist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Define a function for text cleaning\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float) and np.isnan(text): # check if the text is NaN\n",
    "        return \"\" # return empty string\n",
    "    words = word_tokenize(text) # tokenize the text into words using NLTK library \n",
    "    words = [word.lower() for word in words if word.isalnum()] # convert all words to lower case and remove punctuations    \n",
    "    stop_words = set(stopwords.words('english')) # get the stop words from NLTK library \n",
    "    words = [word for word in words if word not in stop_words] # remove stop words from the text\n",
    "    clean_text = ' '.join(words) # join all words into a sentence\n",
    "    return clean_text # return the cleaned text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Apply text cleaning to 'Requirement Description' column for training and validation data\n",
    "train_df['Cleaned_Description'] = train_df['Requirement Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Tokenize and pad sequences for training data\n",
    "max_words = None # We will only consider the top 1000 words in the dataset i want to use the complete dataset\n",
    "tokenizer = Tokenizer(num_words=max_words) # Setup tokenizer using keras Tokenizer class \n",
    "tokenizer.fit_on_texts(train_df['Cleaned_Description']) # fit tokenizer on training data\n",
    "X_train = tokenizer.texts_to_sequences(train_df['Cleaned_Description']) # convert text to sequence of tokens\n",
    "X_train = pad_sequences(X_train, maxlen=100) # pad sequences to make them of equal length\n",
    "print(X_train.shape) # print shape of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6\n",
    "# Label encode 'Requirement Area' for training data\n",
    "area_encoder = LabelEncoder()\n",
    "train_df['Requirement Area (NIST)'] = train_df['Requirement Area (NIST)'].str.lower()\n",
    "y_area_train = area_encoder.fit_transform(train_df['Requirement Area (NIST)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Label encode 'Requirement Bucket(NIST)' for training data\n",
    "bucket_encoder = LabelEncoder()\n",
    "train_df['Requirement Bucket(NIST)'] = train_df['Requirement Bucket(NIST)'].str.lower()\n",
    "y_bucket_train = bucket_encoder.fit_transform(train_df['Requirement Bucket(NIST)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Split data into training and validation sets for evaluation\n",
    "X_train, X_valid, y_area_train, y_area_valid, y_bucket_train, y_bucket_valid = train_test_split(\n",
    "    X_train, y_area_train, y_bucket_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model_file_path = model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9\n",
    "# Check if the model file exists and load it, or create a new model\n",
    "if os.path.exists(model_file_path):\n",
    "    model = tf.keras.models.load_model(model_file_path)\n",
    "else:\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, output_dim=128, input_length=100))\n",
    "    model.add(LSTM(128)) # what  is 128 here? \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    output_area = Dense(len(area_encoder.classes_), activation='softmax', name='output_area')(model.layers[-1].output)\n",
    "    output_bucket = Dense(len(bucket_encoder.classes_), activation='softmax', name='output_bucket')(model.layers[-1].output)\n",
    "    model = Model(inputs=model.input, outputs=[output_area, output_bucket])\n",
    "    model.compile(\n",
    "        loss={'output_area': 'sparse_categorical_crossentropy', 'output_bucket': 'sparse_categorical_crossentropy'},\n",
    "        optimizer='adam',\n",
    "        metrics={'output_area': 'accuracy', 'output_bucket': 'accuracy'}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Train the model on training data with both labels and store the training history\n",
    "history = model.fit(X_train, {'output_area': y_area_train, 'output_bucket': y_bucket_train}, epochs=100, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Save the model to a file in the TensorFlow SavedModel format\n",
    "model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 12\n",
    "# Save the tokenizer using pickle\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Cell 13\n",
    "# Save the encoders using pickle\n",
    "with open('area_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(area_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('bucket_encoder.pickle', 'wb') as handle:\n",
    "    pickle.dump(bucket_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution for 'Area'\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the sorted area labels and their counts\n",
    "area_labels, area_counts = zip(*sorted(zip(train_df['Requirement Area (NIST)'].value_counts().index, train_df['Requirement Area (NIST)'].value_counts()), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(area_labels, area_counts)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# add title with the number of areas and the total number of requirements\n",
    "plt.title('Area Label Distribution')\n",
    "plt.text(0.5, 0.5, str(len(area_labels)) + ' Areas\\n' + str(len(train_df)) + ' Requirements',\n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "plt.xlabel('Area Labels')\n",
    "plt.ylabel('Count')\n",
    "#PRINT COUNT OF EACH AREA\n",
    "for i, v in enumerate(area_counts):\n",
    "    print(area_labels[i], v)\n",
    "   \n",
    "# add count values on top of the bars\n",
    "for i, v in enumerate(area_counts):\n",
    "    plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 15\n",
    "# Visualize label distribution for 'Bucket'\n",
    "plt.figure(figsize=(150, 5))\n",
    "#start from 0 to 150\n",
    "plt.bar(bucket_encoder.classes_, train_df['Requirement Bucket(NIST)'].value_counts())\n",
    "#rotate the labels by 90 degrees\n",
    "plt.xticks(rotation=90)\n",
    "#add title with no.of buckets and total no.of requirements\n",
    "plt.title('Bucket Label Distribution')\n",
    "plt.text(0.5, 0.5, str(len(bucket_encoder.classes_)) + ' Buckets\\n' + str(len(train_df)) + ' Requirements',\n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "#label the x and y axis\n",
    "plt.xlabel('Bucket Labels')\n",
    "plt.ylabel('Count')\n",
    "#display the count of each label\n",
    "# add count values on top of the bars\n",
    "for i, v in enumerate(area_counts):\n",
    "    plt.text(i, v + 10, str(v), ha='center', va='bottom')\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16\n",
    "# Visualize training history for 'Area' and 'Bucket'\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['output_area_accuracy'])\n",
    "plt.plot(history.history['output_bucket_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Area', 'Bucket'], loc='upper left')\n",
    "# add highest accuracy values on top of the lines\n",
    "plt.text(len(history.history['output_area_accuracy']) - 1, history.history['output_area_accuracy'][-1],\n",
    "         str(round(history.history['output_area_accuracy'][-1], 2)), ha='center', va='bottom')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
