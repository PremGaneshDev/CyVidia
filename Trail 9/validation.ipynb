{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2\n",
    "# Load your Excel files for validation\n",
    "validation_file_path = '/Users/PremGanesh/Developer/AI/CyVidia/Input_Data/20-11.xlsx'\n",
    "validation_df = pd.read_excel(validation_file_path)\n",
    "output_file_path = '/Users/PremGanesh/Developer/AI/CyVidia/Output_Data/cl.xlsx'\n",
    "# Model Name\n",
    "model_name = 'trained_model_rbi_jll_nist_scf'\n",
    "# Define column names\n",
    "req_no_col = 'ReqNo'\n",
    "req_area_col = 'Requirement Area'\n",
    "req_area_nist_col = 'Requirement Area (NIST)'\n",
    "req_bucket_nist_col = 'Requirement Bucket(NIST)'\n",
    "predicted_area_col = 'Predicted_Area'\n",
    "suggested_area_col ='Suggested_Area'\n",
    "predicted_bucket_col = 'Predicted_Bucket'\n",
    "key_words_col = 'Key Words'\n",
    "type_col = 'Type'\n",
    "source_type_col = 'Source Type'\n",
    "source_col = 'Source'\n",
    "source_detail_col = 'Source Detail'\n",
    "source_req_id_col = 'Source Requirement ID#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3\n",
    "# Load the saved tokenizer using pickle\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "# Load the encoders from the training process\n",
    "with open('area_encoder.pickle', 'rb') as handle:\n",
    "    area_encoder = pickle.load(handle)\n",
    "with open('bucket_encoder.pickle', 'rb') as handle:\n",
    "    bucket_encoder = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "# Define a function for text cleaning\n",
    "def clean_text(text):\n",
    "    if isinstance(text, float) and np.isnan(text):\n",
    "        return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    clean_text = ' '.join(words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6\n",
    "# Apply text cleaning to 'Requirement Description' column for validation data\n",
    "validation_df['Cleaned_Description'] = validation_df['Requirement Description'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7\n",
    "# Tokenize and pad sequences for validation data\n",
    "max_words = 1000\n",
    "X_val = tokenizer.texts_to_sequences(validation_df['Cleaned_Description'])\n",
    "X_val = pad_sequences(X_val, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8\n",
    "# Load the saved model\n",
    "loaded_model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 1s 41ms/step\n"
     ]
    }
   ],
   "source": [
    "# Cell 9\n",
    "# Predict on validation data using the loaded model\n",
    "y_pred = loaded_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10\n",
    "# Get prediction scores for 'Area' and 'Bucket'\n",
    "prediction_scores_area = np.max(y_pred[0], axis=1)\n",
    "prediction_scores_bucket = np.max(y_pred[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11\n",
    "# Add the prediction scores to the validation DataFrame\n",
    "validation_df['Prediction_Score_Area'] = prediction_scores_area\n",
    "validation_df['Prediction_Score_Bucket'] = prediction_scores_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the to excel file \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12\n",
    "# Define a labeling threshold (e.g., 0.85)\n",
    "labeling_threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13\n",
    "# Initialize lists to store predicted labels and suggested labels\n",
    "predicted_labels_area = []\n",
    "suggested_labels_area = []\n",
    "predicted_labels_bucket = []\n",
    "suggested_labels_bucket = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14\n",
    "# Iterate through predictions and apply labeling/suggestion logic\n",
    "for pred_area, score_area, pred_bucket, score_bucket in zip(\n",
    "    np.argmax(y_pred[0], axis=1),\n",
    "    prediction_scores_area,\n",
    "    np.argmax(y_pred[1], axis=1),\n",
    "    prediction_scores_bucket\n",
    "):\n",
    "    if score_area >= labeling_threshold:\n",
    "        predicted_labels_area.append(area_encoder.inverse_transform([pred_area])[0])\n",
    "        suggested_labels_area.append('')\n",
    "    else:\n",
    "        predicted_labels_area.append('Other')\n",
    "        suggested_labels_area.append(area_encoder.inverse_transform([pred_area])[0])\n",
    "\n",
    "    if score_bucket >= labeling_threshold:\n",
    "        predicted_labels_bucket.append(bucket_encoder.inverse_transform([pred_bucket])[0])\n",
    "        suggested_labels_bucket.append('')\n",
    "    else:\n",
    "        predicted_labels_bucket.append('Other')\n",
    "        suggested_labels_bucket.append(bucket_encoder.inverse_transform([pred_bucket])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15\n",
    "# Add the predicted labels and suggested labels to the validation DataFrame\n",
    "validation_df[predicted_area_col] = predicted_labels_area\n",
    "validation_df[suggested_area_col] = suggested_labels_area\n",
    "validation_df[predicted_bucket_col] = predicted_labels_bucket\n",
    "validation_df['Suggested_Bucket'] = suggested_labels_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results with predicted/suggested labels and scores saved to /Users/PremGanesh/Developer/AI/CyVidia/Output_Data/cl.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Cell 16\n",
    "# Save the validation results DataFrame to an Excel file\n",
    "validation_results_file_path = output_file_path\n",
    "validation_df.to_excel(validation_results_file_path, index=False)\n",
    "print(\"Validation results with predicted/suggested labels and scores saved to\", validation_results_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "df = pd.read_excel(output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Requirement Area (NIST)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/AI/CyVidia/.myenvlocal/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Requirement Area (NIST)'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/PremGanesh/Developer/AI/CyVidia/Trail 9/validation.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/PremGanesh/Developer/AI/CyVidia/Trail%209/validation.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Convert the predicted_area into lower case\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/PremGanesh/Developer/AI/CyVidia/Trail%209/validation.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[req_area_nist_col] \u001b[39m=\u001b[39m df[req_area_nist_col]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/PremGanesh/Developer/AI/CyVidia/Trail%209/validation.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df[req_bucket_nist_col] \u001b[39m=\u001b[39m df[req_bucket_nist_col]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlower()\n",
      "File \u001b[0;32m~/Developer/AI/CyVidia/.myenvlocal/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Developer/AI/CyVidia/.myenvlocal/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Requirement Area (NIST)'"
     ]
    }
   ],
   "source": [
    "# Convert the predicted_area into lower case\n",
    "df[req_area_nist_col] = df[req_area_nist_col].str.lower()\n",
    "df[req_bucket_nist_col] = df[req_bucket_nist_col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_validation(row):\n",
    "    area_validation = \"\"\n",
    "    bucket_validation = \"\"\n",
    "\n",
    "    if not pd.isna(row[req_area_nist_col]) and not pd.isna(row[predicted_area_col]):\n",
    "        if row[req_area_nist_col] == row[predicted_area_col]:\n",
    "            area_validation = 'Correct'\n",
    "        elif row[predicted_area_col] == 'Other' and not pd.isna(row[suggested_area_col]) and row[suggested_area_col] == row[req_area_nist_col]:\n",
    "            area_validation = 'Correct'\n",
    "        else:\n",
    "            area_validation = 'Incorrect'\n",
    "    else:\n",
    "        area_validation = 'Empty'\n",
    "\n",
    "    if not pd.isna(row[req_bucket_nist_col]) and not pd.isna(row[predicted_bucket_col]):\n",
    "        if row[req_bucket_nist_col] == row[predicted_bucket_col]:\n",
    "            bucket_validation = 'Correct'\n",
    "        elif row[predicted_bucket_col] == 'Other' and not pd.isna(row['Suggested_Bucket']) and row['Suggested_Bucket'] == row['Requirement Bucket(NIST)']:\n",
    "            bucket_validation = 'Correct'\n",
    "        else:\n",
    "            bucket_validation = 'Incorrect'\n",
    "    else:\n",
    "        bucket_validation = 'Empty'\n",
    "\n",
    "    return area_validation, bucket_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to create new columns\n",
    "df['Area Validation'], df['Bucket Validation'] = zip(*df.apply(calculate_validation, axis=1))\n",
    "# add the validation area and bucket to the excel file and save it to the output    \n",
    "df.to_excel(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counts for 'Area Validation' and 'Bucket Validation'\n",
    "area_validation_counts = df['Area Validation'].value_counts()\n",
    "bucket_validation_counts = df['Bucket Validation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percentages for 'Area Validation Accuracy' and 'Bucket Validation Accuracy'\n",
    "area_validation_accuracy = (area_validation_counts.get('Correct', 0) / (area_validation_counts.sum())) * 100\n",
    "bucket_validation_accuracy = (bucket_validation_counts.get('Correct', 0) / (bucket_validation_counts.sum())) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the counts and percentages\n",
    "validation_summary = {\n",
    "    'Total Count': len(df),\n",
    "    'Area Validation Correct Count': area_validation_counts.get('Correct', 0),\n",
    "    'Area Validation Incorrect Count': area_validation_counts.get('Incorrect', 0),\n",
    "    'Area Validation Empty Count': area_validation_counts.get('Empty', 0),\n",
    "    'Bucket Validation Correct Count': bucket_validation_counts.get('Correct', 0),\n",
    "    'Bucket Validation Incorrect Count': bucket_validation_counts.get('Incorrect', 0),\n",
    "    'Bucket Validation Empty Count': bucket_validation_counts.get('Empty', 0),\n",
    "    'Area Validation Accuracy': area_validation_accuracy,\n",
    "    'Bucket Validation Accuracy': bucket_validation_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the differences between Predicted and Suggested Areas\n",
    "differently_mapped = df[df['Area Validation'] == 'Incorrect'][[predicted_area_col, suggested_area_col]]\n",
    "differently_mapped['Area Difference'] = differently_mapped[predicted_area_col] + \" (Predicted) vs. \" + differently_mapped[suggested_area_col] + \" (Suggested\"\n",
    "difference_counts = differently_mapped['Area Difference'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.bar(difference_counts.index, difference_counts, color='blue')\n",
    "\n",
    "plt.title('Differences between Predicted and Suggested Areas')\n",
    "plt.xlabel('Differences')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Create a DataFrame to store the correctness and colors\n",
    "correctness_colors = pd.DataFrame({\n",
    "    'Correctness': df['Area Validation'],\n",
    "    'Color': df['Area Validation'].apply(lambda x: 'green' if x == 'Correct' else 'red')\n",
    "})\n",
    "\n",
    "# Scatter plot to visualize correctness\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter the data points\n",
    "scatter = plt.scatter(df.index, df[predicted_area_col], c=correctness_colors['Color'], marker='o')\n",
    "\n",
    "plt.title('Correct vs. Incorrect Mapping of Areas')\n",
    "plt.xlabel('Data Point (Index)')\n",
    "plt.ylabel('Area Labels')\n",
    "\n",
    "# Add labels to the data points\n",
    "for i, label in enumerate(df.index):\n",
    "    plt.annotate(label, (label, df[predicted_area_col][i]), fontsize=8, ha='center', va='center', color='black')\n",
    "\n",
    "# Create a legend\n",
    "correct_patch = mpatches.Patch(color='green', label='Correct')\n",
    "incorrect_patch = mpatches.Patch(color='red', label='Incorrect')\n",
    "plt.legend(handles=[correct_patch, incorrect_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Define the labeling threshold\n",
    "labeling_threshold = 0.5  # Adjust this threshold as needed\n",
    "\n",
    "# Determine correctness based on prediction scores and labeling threshold\n",
    "correctness = (df['Prediction_Score_Area'] >= labeling_threshold)\n",
    "\n",
    "# Scatter plot to visualize correctness based on threshold\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Scatter the data points\n",
    "plt.scatter(df['Prediction_Score_Area'], df.index, c=correctness, cmap='coolwarm', marker='o')\n",
    "\n",
    "plt.title('Accuracy Based on Prediction Scores and Labeling Threshold')\n",
    "plt.xlabel('Prediction Score for Area')\n",
    "plt.ylabel('Data Point')\n",
    "plt.yticks([])\n",
    "\n",
    "# Create a colorbar for the scatter plot\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Correctness')\n",
    "\n",
    "# Add a legend\n",
    "correct_patch = mpatches.Patch(color='blue', label='Correct')\n",
    "incorrect_patch = mpatches.Patch(color='red', label='Incorrect')\n",
    "plt.legend(handles=[correct_patch, incorrect_patch])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame and transpose it\n",
    "summary_df = pd.DataFrame(validation_summary, index=[0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "summary_df = summary_df.rename(columns={0: 'Count'})\n",
    "# Save the validation summary to an Excel file\n",
    "validation_summary_file_path = output_file_path+'_summary.xlsx'\n",
    "summary_df.to_excel(validation_summary_file_path)\n",
    "print(\"Validation summary saved to\", validation_summary_file_path)\n",
    "print('output validation file path ' ,output_file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
